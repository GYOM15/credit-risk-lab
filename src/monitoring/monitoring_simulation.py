# -*- coding: utf-8 -*-
"""monitoring_simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hx-680h8SACCtYoLB0Gfk9oa3NyqpiMl

# **SIMULATION DE MONITORING**
"""

# Si pas encore monté
# from google.colab import drive
# drive.mount('/content/drive')

"""
Simulation de Monitoring - Détection de Data Drift et Prediction Drift
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ks_2samp, chi2_contingency
import warnings
warnings.filterwarnings('ignore')

# ==============================================================================
# CLASSE DE MONITORING
# ==============================================================================

class ModelMonitor:
    """
    Surveillance du modèle en production.
    Détecte les dérives (drift) dans les données et les prédictions.
    """

    def __init__(self, X_train_baseline, y_train_baseline, predictions_baseline):
        """
        Initialise avec les données de référence (baseline).

        Parameters
        ----------
        X_train_baseline : DataFrame
            Données d'entraînement de référence
        y_train_baseline : Series
            Target de référence
        predictions_baseline : array
            Prédictions de référence
        """
        self.X_baseline = X_train_baseline
        self.y_baseline = y_train_baseline
        self.predictions_baseline = predictions_baseline

        # Seuils d'alerte
        self.data_drift_threshold = 0.05  # p-value KS test
        self.prediction_drift_threshold = 0.10  # 10% de changement

    def check_data_drift(self, X_production):
        """
        Détecte le data drift sur les features numériques.
        Utilise le test de Kolmogorov-Smirnov.

        Parameters
        ----------
        X_production : DataFrame
            Nouvelles données en production

        Returns
        -------
        DataFrame : Rapport de drift par feature
        """
        drift_report = []

        # Sélectionner seulement les colonnes numériques communes
        numeric_cols = self.X_baseline.select_dtypes(include=[np.number]).columns
        common_cols = [col for col in numeric_cols if col in X_production.columns]

        for feature in common_cols:
            # Test KS pour comparer les distributions
            stat, p_value = ks_2samp(
                self.X_baseline[feature].dropna(),
                X_production[feature].dropna()
            )

            # Calculer le changement de moyenne
            mean_baseline = self.X_baseline[feature].mean()
            mean_production = X_production[feature].mean()
            mean_change = ((mean_production - mean_baseline) / mean_baseline) * 100

            # Déterminer si drift
            has_drift = p_value < self.data_drift_threshold

            drift_report.append({
                'Feature': feature,
                'KS_Statistic': stat,
                'P_Value': p_value,
                'Mean_Baseline': mean_baseline,
                'Mean_Production': mean_production,
                'Mean_Change_%': mean_change,
                'Drift_Detected': has_drift,
                'Severity': self._get_severity(p_value)
            })

        return pd.DataFrame(drift_report).sort_values('P_Value')

    def check_prediction_drift(self, predictions_production):
        """
        Détecte le drift dans les prédictions.

        Parameters
        ----------
        predictions_production : array
            Nouvelles prédictions en production

        Returns
        -------
        dict : Rapport de drift des prédictions
        """
        mean_baseline = self.predictions_baseline.mean()
        mean_production = predictions_production.mean()

        change_pct = abs(mean_production - mean_baseline) / mean_baseline

        # Distribution des prédictions
        std_baseline = self.predictions_baseline.std()
        std_production = predictions_production.std()

        # Test KS sur les prédictions
        stat, p_value = ks_2samp(self.predictions_baseline, predictions_production)

        has_drift = change_pct > self.prediction_drift_threshold

        return {
            'mean_baseline': mean_baseline,
            'mean_production': mean_production,
            'change_pct': change_pct * 100,
            'std_baseline': std_baseline,
            'std_production': std_production,
            'ks_statistic': stat,
            'ks_p_value': p_value,
            'drift_detected': has_drift,
            'severity': 'HIGH' if change_pct > 0.20 else 'MEDIUM' if change_pct > 0.10 else 'LOW'
        }

    def check_target_drift(self, y_production):
        """
        Détecte le drift dans la target (si disponible en production).

        Parameters
        ----------
        y_production : Series
            Target en production

        Returns
        -------
        dict : Rapport de drift de la target
        """
        # Calculer les proportions
        baseline_dist = self.y_baseline.value_counts(normalize=True).sort_index()
        production_dist = y_production.value_counts(normalize=True).sort_index()

        # Chi-square test pour variables catégorielles
        contingency_table = pd.DataFrame({
            'Baseline': self.y_baseline.value_counts(),
            'Production': y_production.value_counts()
        }).fillna(0)

        chi2, p_value, dof, expected = chi2_contingency(contingency_table)

        return {
            'baseline_distribution': baseline_dist.to_dict(),
            'production_distribution': production_dist.to_dict(),
            'chi2_statistic': chi2,
            'p_value': p_value,
            'drift_detected': p_value < 0.05,
            'severity': 'HIGH' if p_value < 0.01 else 'MEDIUM' if p_value < 0.05 else 'LOW'
        }

    def _get_severity(self, p_value):
        """Détermine la sévérité du drift selon la p-value."""
        if p_value < 0.01:
            return 'HIGH'
        elif p_value < 0.05:
            return 'MEDIUM'
        else:
            return 'LOW'

    def generate_monitoring_report(self, X_prod, y_prod=None, predictions_prod=None):
        """
        Génère un rapport complet de monitoring.

        Parameters
        ----------
        X_prod : DataFrame
            Données en production
        y_prod : Series, optional
            Target en production
        predictions_prod : array, optional
            Prédictions en production

        Returns
        -------
        dict : Rapport complet
        """
        from datetime import datetime

        report = {
            'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'n_samples_baseline': len(self.X_baseline),
            'n_samples_production': len(X_prod)
        }

        # Data drift
        print("Analyse du Data Drift...")
        report['data_drift'] = self.check_data_drift(X_prod)

        # Prediction drift
        if predictions_prod is not None:
            print("Analyse du Prediction Drift...")
            report['prediction_drift'] = self.check_prediction_drift(predictions_prod)

        # Target drift
        if y_prod is not None:
            print("Analyse du Target Drift...")
            report['target_drift'] = self.check_target_drift(y_prod)

        return report

    def visualize_drift(self, X_prod, predictions_prod=None, save_path=None):
        """
        Visualise les drifts détectés.

        Parameters
        ----------
        X_prod : DataFrame
            Données en production
        predictions_prod : array, optional
            Prédictions en production
        save_path : str, optional
            Chemin pour sauvegarder les graphiques
        """
        # Data drift
        drift_report = self.check_data_drift(X_prod)
        features_with_drift = drift_report[drift_report['Drift_Detected']]['Feature'].tolist()

        if not features_with_drift:
            print("Aucun drift détecté dans les features")
            return

        # Limiter à 6 features pour la visualisation
        features_to_plot = features_with_drift[:6]

        n_features = len(features_to_plot)
        n_cols = 3
        n_rows = (n_features + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
        axes = axes.flatten() if n_features > 1 else [axes]

        for idx, feature in enumerate(features_to_plot):
            ax = axes[idx]

            # Distributions
            ax.hist(self.X_baseline[feature].dropna(),
                   bins=30, alpha=0.5, label='Baseline (Train)',
                   color='blue', density=True)
            ax.hist(X_prod[feature].dropna(),
                   bins=30, alpha=0.5, label='Production',
                   color='red', density=True)

            # KS test p-value
            p_val = drift_report[drift_report['Feature'] == feature]['P_Value'].values[0]

            ax.set_title(f'{feature}\nDrift Detected (p={p_val:.4f})', fontweight='bold')
            ax.set_xlabel('Valeur')
            ax.set_ylabel('Densité')
            ax.legend()
            ax.grid(alpha=0.3)

        # Masquer les axes vides
        for idx in range(n_features, len(axes)):
            axes[idx].axis('off')

        plt.tight_layout()

        if save_path:
            plt.savefig(f'{save_path}/data_drift_visualization.png', dpi=150, bbox_inches='tight')
            print(f"Graphique sauvegardé : {save_path}/data_drift_visualization.png")

        plt.show()

        # Prediction drift (si disponible)
        if predictions_prod is not None:
            self._visualize_prediction_drift(predictions_prod, save_path)

    def _visualize_prediction_drift(self, predictions_prod, save_path=None):
        """Visualise le drift des prédictions."""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Distribution des prédictions
        axes[0].hist(self.predictions_baseline, bins=30, alpha=0.5,
                    label='Baseline', color='blue', density=True)
        axes[0].hist(predictions_prod, bins=30, alpha=0.5,
                    label='Production', color='red', density=True)
        axes[0].set_title('Distribution des Prédictions', fontweight='bold')
        axes[0].set_xlabel('Probabilité de défaut')
        axes[0].set_ylabel('Densité')
        axes[0].legend()
        axes[0].grid(alpha=0.3)

        # Box plots
        data_to_plot = [self.predictions_baseline, predictions_prod]
        axes[1].boxplot(data_to_plot, labels=['Baseline', 'Production'])
        axes[1].set_title('Comparaison des Prédictions', fontweight='bold')
        axes[1].set_ylabel('Probabilité de défaut')
        axes[1].grid(alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(f'{save_path}/prediction_drift_visualization.png', dpi=150, bbox_inches='tight')
            print(f"Graphique sauvegardé : {save_path}/prediction_drift_visualization.png")

        plt.show()


# ==============================================================================
# FONCTION DE SIMULATION
# ==============================================================================

def simulate_drift(X_baseline, drift_type='moderate', features_to_drift=None):
    """
    Simule un drift dans les données.

    Parameters
    ----------
    X_baseline : DataFrame
        Données de référence
    drift_type : str
        Type de drift : 'light', 'moderate', 'severe'
    features_to_drift : list, optional
        Features spécifiques à faire drifter

    Returns
    -------
    DataFrame : Données avec drift simulé
    """
    X_drift = X_baseline.copy()

    # Paramètres selon le type de drift
    drift_params = {
        'light': {'shift': 0.1, 'scale': 1.05},
        'moderate': {'shift': 0.3, 'scale': 1.15},
        'severe': {'shift': 0.5, 'scale': 1.30}
    }

    params = drift_params.get(drift_type, drift_params['moderate'])

    # Features numériques
    numeric_cols = X_drift.select_dtypes(include=[np.number]).columns

    if features_to_drift is None:
        # Sélectionner aléatoirement 30% des features
        n_features = max(1, int(len(numeric_cols) * 0.3))
        features_to_drift = np.random.choice(numeric_cols, size=n_features, replace=False)

    # Appliquer le drift
    for feature in features_to_drift:
        if feature in numeric_cols:
            # Shift de la moyenne
            mean_val = X_drift[feature].mean()
            X_drift[feature] = X_drift[feature] + mean_val * params['shift']

            # Augmentation de la variance
            X_drift[feature] = X_drift[feature] * params['scale']

    print(f"Drift simulé sur {len(features_to_drift)} features : {list(features_to_drift)}")

    return X_drift


# ==============================================================================
# EXEMPLE D'UTILISATION
# ==============================================================================

def run_monitoring_simulation(X_train, y_train, predictions_train,
                              X_test, y_test=None, predictions_test=None,
                              drift_type='moderate', save_path='./outputs'):
    """
    Execute une simulation complète de monitoring.

    Parameters
    ----------
    X_train : DataFrame
        Données d'entraînement (baseline)
    y_train : Series
        Target d'entraînement
    predictions_train : array
        Prédictions sur le train
    X_test : DataFrame
        Données de test (avant drift)
    y_test : Series, optional
        Target de test
    predictions_test : array, optional
        Prédictions sur le test
    drift_type : str
        Type de drift à simuler
    save_path : str
        Chemin pour sauvegarder les résultats
    """
    print("="*80)
    print("SIMULATION DE MONITORING - DÉTECTION DE DRIFT")
    print("="*80)

    # Créer le moniteur
    print("\n1. Initialisation du moniteur avec les données baseline...")
    monitor = ModelMonitor(X_train, y_train, predictions_train)
    print(f"   Baseline : {len(X_train)} échantillons")

    # Simuler du drift
    print(f"\n2. Simulation d'un drift {drift_type.upper()}...")
    X_drift = simulate_drift(X_test, drift_type=drift_type)

    # Si on a des prédictions, les régénérer sur les données driftées
    # (Dans un vrai cas, ce serait le modèle qui prédit sur les nouvelles données)
    predictions_drift = predictions_test  # Simplifié pour la démo

    # Générer le rapport
    print("\n3. Génération du rapport de monitoring...")
    report = monitor.generate_monitoring_report(
        X_prod=X_drift,
        y_prod=y_test,
        predictions_prod=predictions_drift
    )

    # Afficher les résultats
    print("\n" + "="*80)
    print("RÉSULTATS DU MONITORING")
    print("="*80)

    # Data Drift
    print("\n--- DATA DRIFT ---")
    drift_summary = report['data_drift']
    drift_detected = drift_summary[drift_summary['Drift_Detected']]

    if len(drift_detected) > 0:
        print(f"\nALERTE : {len(drift_detected)} feature(s) avec drift détecté :")
        for idx, row in drift_detected.iterrows():
            print(f"  - {row['Feature']} : p-value={row['P_Value']:.4f}, "
                  f"changement moyen={row['Mean_Change_%']:.1f}%, "
                  f"sévérité={row['Severity']}")
    else:
        print("\nOK : Aucun drift détecté dans les features")

    # Prediction Drift
    if 'prediction_drift' in report:
        print("\n--- PREDICTION DRIFT ---")
        pred_drift = report['prediction_drift']

        if pred_drift['drift_detected']:
            print(f"ALERTE : Drift dans les prédictions détecté")
            print(f"  - Moyenne baseline : {pred_drift['mean_baseline']:.4f}")
            print(f"  - Moyenne production : {pred_drift['mean_production']:.4f}")
            print(f"  - Changement : {pred_drift['change_pct']:.1f}%")
            print(f"  - Sévérité : {pred_drift['severity']}")
        else:
            print("OK : Pas de drift significatif dans les prédictions")

    # Target Drift
    if 'target_drift' in report:
        print("\n--- TARGET DRIFT ---")
        target_drift = report['target_drift']

        if target_drift['drift_detected']:
            print(f"ALERTE : Drift dans la distribution de la target")
            print(f"  - p-value : {target_drift['p_value']:.4f}")
            print(f"  - Sévérité : {target_drift['severity']}")
        else:
            print("OK : Distribution de la target stable")

    # Visualisations
    print("\n4. Génération des visualisations...")
    monitor.visualize_drift(X_drift, predictions_drift, save_path=save_path)

    # Recommandations
    print("\n" + "="*80)
    print("RECOMMANDATIONS")
    print("="*80)

    total_drift = len(drift_summary[drift_summary['Drift_Detected']])

    if total_drift == 0:
        print("\nModèle stable - Monitoring de routine recommandé")
    elif total_drift <= 2:
        print("\nDrift léger détecté :")
        print("  1. Surveiller de près les prochaines semaines")
        print("  2. Analyser les causes du drift")
        print("  3. Évaluer les performances réelles du modèle")
    else:
        print("\nDrift significatif détecté :")
        print("  1. URGENT : Évaluer les performances du modèle sur données récentes")
        print("  2. Investiguer les causes du drift (changement métier, données, etc.)")
        print("  3. Considérer un réentraînement du modèle")
        print("  4. Mettre en place des alertes automatiques")

    print("\n" + "="*80)
    print("MONITORING TERMINÉ")
    print("="*80)

    return report, monitor